# üìä M√©tricas de Avalia√ß√£o do Sistema de Recomenda√ß√£o

## Vis√£o Geral

O sistema agora calcula **m√∫ltiplas m√©tricas** durante o treinamento para avaliar a qualidade do modelo de forma abrangente. Todas as m√©tricas s√£o registradas automaticamente no MLflow.

---

## üéØ M√©tricas Implementadas

### 1. Precision@K

**O que mede:** Quantos dos itens recomendados s√£o realmente relevantes.

**F√≥rmula:** `Precision@K = (Itens relevantes nos top K) / K`

**Interpreta√ß√£o:**
- **Alto (pr√≥ximo de 1.0)**: A maioria das recomenda√ß√µes √© relevante
- **Baixo (pr√≥ximo de 0.0)**: Muitas recomenda√ß√µes n√£o s√£o relevantes

**Valores calculados:**
- `test_precision@5`: Precision nas top 5 recomenda√ß√µes
- `test_precision@10`: Precision nas top 10 recomenda√ß√µes
- `test_precision@20`: Precision nas top 20 recomenda√ß√µes

**Exemplo:**
- Se voc√™ recomenda 10 estabelecimentos e 7 s√£o relevantes: Precision@10 = 0.7

---

### 2. Recall@K

**O que mede:** Quantos dos itens relevantes foram recuperados nas recomenda√ß√µes.

**F√≥rmula:** `Recall@K = (Itens relevantes nos top K) / (Total de itens relevantes)`

**Interpreta√ß√£o:**
- **Alto (pr√≥ximo de 1.0)**: O modelo encontra a maioria dos itens relevantes
- **Baixo (pr√≥ximo de 0.0)**: O modelo perde muitos itens relevantes

**Valores calculados:**
- `test_recall@5`: Recall nas top 5 recomenda√ß√µes
- `test_recall@10`: Recall nas top 10 recomenda√ß√µes

**Exemplo:**
- Se existem 20 estabelecimentos relevantes e voc√™ recomenda 10, encontrando 8: Recall@10 = 0.4

---

### 3. F1-Score@K

**O que mede:** M√©dia harm√¥nica entre Precision e Recall (balanceamento).

**F√≥rmula:** `F1@K = 2 √ó (Precision@K √ó Recall@K) / (Precision@K + Recall@K)`

**Interpreta√ß√£o:**
- **Alto (pr√≥ximo de 1.0)**: Boa combina√ß√£o de Precision e Recall
- **Baixo (pr√≥ximo de 0.0)**: Um dos dois (ou ambos) est√° baixo

**Valores calculados:**
- `test_f1@5`: F1-Score nas top 5 recomenda√ß√µes
- `test_f1@10`: F1-Score nas top 10 recomenda√ß√µes

**Quando usar:**
- Quando voc√™ quer balancear Precision e Recall
- √ötil quando h√° trade-off entre encontrar mais itens (Recall) vs. garantir relev√¢ncia (Precision)

---

### 4. AUC (Area Under the ROC Curve)

**O que mede:** Capacidade do modelo de distinguir entre itens relevantes e n√£o relevantes.

**F√≥rmula:** √Årea sob a curva ROC (Receiver Operating Characteristic)

**Interpreta√ß√£o:**
- **1.0**: Perfeito - modelo sempre classifica corretamente
- **0.5**: Aleat√≥rio - n√£o melhor que chute
- **< 0.5**: Pior que aleat√≥rio

**Valores calculados:**
- `train_auc`: AUC no conjunto de treino
- `test_auc`: AUC no conjunto de teste

**Quando usar:**
- Avaliar qualidade geral do modelo
- Comparar diferentes algoritmos
- Detectar overfitting (train_auc muito maior que test_auc)

---

### 5. MRR (Mean Reciprocal Rank)

**O que mede:** Posi√ß√£o m√©dia do primeiro item relevante na lista de recomenda√ß√µes.

**F√≥rmula:** `MRR = (1 / posi√ß√£o_do_primeiro_relevante) / n√∫mero_de_usu√°rios`

**Interpreta√ß√£o:**
- **1.0**: Primeiro item sempre √© relevante
- **0.5**: Primeiro item relevante aparece em m√©dia na posi√ß√£o 2
- **0.0**: Nenhum item relevante encontrado

**Valores calculados:**
- `train_mrr`: MRR no conjunto de treino
- `test_mrr`: MRR no conjunto de teste

**Quando usar:**
- Quando a posi√ß√£o do primeiro item relevante √© importante
- √ötil para sistemas onde o usu√°rio v√™ apenas as primeiras recomenda√ß√µes

**Exemplo:**
- Se o primeiro item relevante aparece na posi√ß√£o 3: MRR = 1/3 = 0.33

---

## üìà Compara√ß√£o de M√©tricas

### Qual M√©trica Usar?

| Situa√ß√£o | M√©trica Recomendada | Por qu√™? |
|----------|-------------------|----------|
| **Qualidade geral** | AUC | Mede capacidade de distinguir relevante/n√£o relevante |
| **Top da lista** | Precision@5, MRR | Foco nas primeiras recomenda√ß√µes |
| **Cobertura** | Recall@10 | Quantos itens relevantes foram encontrados |
| **Balanceamento** | F1@10 | Combina Precision e Recall |
| **Diferentes tamanhos** | Precision@5, @10, @20 | Ver como performance varia com tamanho da lista |

---

## üéØ Interpreta√ß√£o Pr√°tica

### Cen√°rio 1: Alta Precision, Baixo Recall

```
Precision@10: 0.9
Recall@10: 0.2
```

**Significado:**
- ‚úÖ Recomenda√ß√µes s√£o muito relevantes (90%)
- ‚ùå Mas encontra poucos itens relevantes (20%)
- **A√ß√£o:** Modelo √© conservador, precisa ser mais explorat√≥rio

### Cen√°rio 2: Baixa Precision, Alta Recall

```
Precision@10: 0.3
Recall@10: 0.8
```

**Significado:**
- ‚ùå Muitas recomenda√ß√µes n√£o s√£o relevantes (30%)
- ‚úÖ Mas encontra a maioria dos itens relevantes (80%)
- **A√ß√£o:** Modelo √© muito explorat√≥rio, precisa ser mais preciso

### Cen√°rio 3: Balanceado (Ideal)

```
Precision@10: 0.7
Recall@10: 0.6
F1@10: 0.65
```

**Significado:**
- ‚úÖ Boa precis√£o (70% relevantes)
- ‚úÖ Boa cobertura (60% dos relevantes encontrados)
- ‚úÖ Balanceado (F1 = 0.65)

---

## üîç An√°lise no MLflow

### Como Comparar Modelos

1. **Acesse MLflow UI:** `mlflow ui`
2. **Compare runs:** Selecione m√∫ltiplos experimentos
3. **Analise m√©tricas:**
   - **Precision@10**: Qual modelo tem mais recomenda√ß√µes relevantes?
   - **Recall@10**: Qual modelo encontra mais itens relevantes?
   - **F1@10**: Qual modelo tem melhor balanceamento?
   - **AUC**: Qual modelo tem melhor capacidade geral?

### Sele√ß√£o do Melhor Modelo

O sistema usa **test_precision@10** como m√©trica principal para selecionar o melhor modelo, mas voc√™ pode:

1. **Ver todas as m√©tricas** no MLflow
2. **Comparar manualmente** diferentes aspectos
3. **Escolher modelo** baseado na m√©trica mais importante para seu caso

---

## üìä Exemplo de Sa√≠da

Ap√≥s treinar, voc√™ ver√° no console:

```
üìä MLflow: M√©tricas registradas:
   Precision@10: 0.4523, Recall@10: 0.3821
   AUC: 0.7821, MRR: 0.6234
   F1@10: 0.4134
```

E no MLflow UI, todas as m√©tricas estar√£o dispon√≠veis para compara√ß√£o!

---

## üéì Refer√™ncias

- **Precision & Recall**: M√©tricas cl√°ssicas de recupera√ß√£o de informa√ß√£o
- **AUC**: Padr√£o em classifica√ß√£o bin√°ria
- **MRR**: Comum em sistemas de busca e recomenda√ß√£o
- **F1-Score**: Balanceamento entre Precision e Recall

---

## üí° Dicas

1. **N√£o foque apenas em uma m√©trica**: Use m√∫ltiplas para ter vis√£o completa
2. **Considere o contexto**: Precision pode ser mais importante em alguns casos, Recall em outros
3. **Compare com baseline**: Um modelo aleat√≥rio tem Precision@10 ‚âà 0.1 (se 10% dos itens s√£o relevantes)
4. **Monitore overfitting**: Se train_auc >> test_auc, o modelo est√° decorando os dados
5. **Use F1 quando houver trade-off**: Se Precision e Recall est√£o em conflito, F1 ajuda a balancear

---

**√öltima atualiza√ß√£o:** Sistema agora calcula 10+ m√©tricas automaticamente durante o treinamento! üéâ

